{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a24cfc7",
   "metadata": {},
   "source": [
    "## 1. Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c111a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, math, time, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9595e744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models, datasets, transforms\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b037b",
   "metadata": {},
   "source": [
    "## 2. Training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a04f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train: 50,000, Valid: 10,000\n"
     ]
    }
   ],
   "source": [
    "# Set up train and validation datasets\n",
    "norm_stats = ((0.5071, 0.4866, 0.4409),(0.2009, 0.1984, 0.2023)) # CIFAR100 training set normalization constants\n",
    "R = 384\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.AutoAugment(policy = transforms.autoaugment.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(R),\n",
    "    transforms.ToTensor(), # Also standardizes to range [0,1]\n",
    "    transforms.Normalize(*norm_stats),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(R),\n",
    "    transforms.ToTensor(), # Also standardizes to range [0,1]\n",
    "    transforms.Normalize(*norm_stats),\n",
    "])\n",
    "\n",
    "ROOT = '/data/cifar100'\n",
    "train_dataset = datasets.CIFAR100(root=ROOT, train=True, transform=train_transform, download=True)\n",
    "\n",
    "# Hold-out this data for final evaluation\n",
    "valid_dataset = datasets.CIFAR100(root=ROOT, train=False, transform=valid_transform, download=True)\n",
    "\n",
    "print(f'Train: {len(train_dataset):,.0f}, Valid: {len(valid_dataset):,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef573d8",
   "metadata": {},
   "source": [
    "## 3. Run training and evaluation routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc69214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated effnetv2 model with 20,305,588 params.\n",
      "EPOCH 0, TLOSS 4.449, VLOSS 4.120, TOP1 6.14, TOP5 22.60, TIME 92.494\n",
      "EPOCH 1, TLOSS 4.246, VLOSS 3.934, TOP1 8.61, TOP5 29.02, TIME 90.677\n",
      "EPOCH 2, TLOSS 4.112, VLOSS 3.751, TOP1 10.91, TOP5 33.70, TIME 90.634\n",
      "EPOCH 3, TLOSS 3.910, VLOSS 3.533, TOP1 15.30, TOP5 39.85, TIME 90.822\n",
      "EPOCH 4, TLOSS 3.686, VLOSS 3.106, TOP1 21.77, TOP5 51.74, TIME 90.593\n",
      "EPOCH 5, TLOSS 3.481, VLOSS 2.948, TOP1 25.07, TOP5 56.38, TIME 90.990\n",
      "EPOCH 6, TLOSS 3.275, VLOSS 2.651, TOP1 31.27, TOP5 63.75, TIME 91.089\n",
      "EPOCH 7, TLOSS 3.104, VLOSS 2.351, TOP1 37.53, TOP5 70.56, TIME 90.890\n",
      "EPOCH 8, TLOSS 2.958, VLOSS 2.165, TOP1 41.18, TOP5 74.27, TIME 91.117\n",
      "EPOCH 9, TLOSS 2.850, VLOSS 2.090, TOP1 43.11, TOP5 76.18, TIME 90.968\n",
      "EPOCH 10, TLOSS 2.721, VLOSS 1.864, TOP1 48.87, TOP5 79.90, TIME 91.210\n",
      "EPOCH 11, TLOSS 2.631, VLOSS 1.817, TOP1 49.70, TOP5 80.81, TIME 91.301\n",
      "EPOCH 12, TLOSS 2.531, VLOSS 1.668, TOP1 52.84, TOP5 83.57, TIME 91.139\n",
      "EPOCH 13, TLOSS 2.467, VLOSS 1.611, TOP1 54.37, TOP5 84.12, TIME 91.041\n",
      "EPOCH 14, TLOSS 2.385, VLOSS 1.567, TOP1 55.50, TOP5 85.46, TIME 91.216\n",
      "EPOCH 15, TLOSS 2.336, VLOSS 1.471, TOP1 58.08, TOP5 86.44, TIME 91.056\n",
      "EPOCH 16, TLOSS 2.242, VLOSS 1.449, TOP1 58.60, TOP5 87.07, TIME 91.121\n",
      "EPOCH 17, TLOSS 2.191, VLOSS 1.415, TOP1 59.71, TOP5 87.36, TIME 91.143\n",
      "EPOCH 18, TLOSS 2.146, VLOSS 1.347, TOP1 61.37, TOP5 88.48, TIME 91.371\n",
      "EPOCH 19, TLOSS 2.098, VLOSS 1.316, TOP1 62.27, TOP5 88.93, TIME 91.084\n",
      "EPOCH 20, TLOSS 2.038, VLOSS 1.248, TOP1 64.04, TOP5 89.77, TIME 91.118\n",
      "EPOCH 21, TLOSS 1.993, VLOSS 1.229, TOP1 64.62, TOP5 90.15, TIME 91.311\n",
      "EPOCH 22, TLOSS 1.968, VLOSS 1.241, TOP1 64.47, TOP5 89.87, TIME 91.185\n",
      "EPOCH 23, TLOSS 1.918, VLOSS 1.165, TOP1 65.85, TOP5 90.90, TIME 91.146\n",
      "EPOCH 24, TLOSS 1.888, VLOSS 1.182, TOP1 65.91, TOP5 90.74, TIME 91.259\n",
      "EPOCH 25, TLOSS 1.859, VLOSS 1.154, TOP1 67.06, TOP5 91.08, TIME 91.339\n",
      "EPOCH 26, TLOSS 1.805, VLOSS 1.141, TOP1 67.12, TOP5 91.39, TIME 91.333\n",
      "EPOCH 27, TLOSS 1.758, VLOSS 1.095, TOP1 68.70, TOP5 91.99, TIME 91.258\n",
      "EPOCH 28, TLOSS 1.737, VLOSS 1.130, TOP1 68.21, TOP5 92.02, TIME 91.174\n",
      "EPOCH 29, TLOSS 1.724, VLOSS 1.097, TOP1 68.94, TOP5 91.86, TIME 91.291\n",
      "EPOCH 30, TLOSS 1.706, VLOSS 1.058, TOP1 69.83, TOP5 92.53, TIME 91.024\n",
      "EPOCH 31, TLOSS 1.669, VLOSS 1.030, TOP1 70.58, TOP5 92.75, TIME 91.066\n",
      "EPOCH 32, TLOSS 1.621, VLOSS 1.046, TOP1 70.51, TOP5 92.58, TIME 90.833\n",
      "EPOCH 33, TLOSS 1.588, VLOSS 1.044, TOP1 70.96, TOP5 92.80, TIME 91.117\n",
      "EPOCH 34, TLOSS 1.602, VLOSS 1.031, TOP1 70.86, TOP5 92.66, TIME 91.368\n",
      "EPOCH 35, TLOSS 1.543, VLOSS 1.022, TOP1 71.22, TOP5 92.77, TIME 91.286\n",
      "EPOCH 36, TLOSS 1.530, VLOSS 0.998, TOP1 72.20, TOP5 92.74, TIME 91.126\n",
      "EPOCH 37, TLOSS 1.514, VLOSS 1.040, TOP1 71.16, TOP5 92.55, TIME 91.047\n",
      "EPOCH 38, TLOSS 1.485, VLOSS 0.999, TOP1 72.44, TOP5 92.96, TIME 91.125\n",
      "EPOCH 39, TLOSS 1.457, VLOSS 1.016, TOP1 72.01, TOP5 93.08, TIME 91.082\n",
      "EPOCH 40, TLOSS 1.446, VLOSS 1.031, TOP1 71.64, TOP5 93.25, TIME 91.070\n",
      "EPOCH 41, TLOSS 1.396, VLOSS 0.987, TOP1 72.77, TOP5 93.25, TIME 91.142\n",
      "EPOCH 42, TLOSS 1.404, VLOSS 1.005, TOP1 72.75, TOP5 93.34, TIME 91.046\n",
      "EPOCH 43, TLOSS 1.379, VLOSS 1.005, TOP1 72.89, TOP5 93.50, TIME 91.193\n",
      "EPOCH 44, TLOSS 1.375, VLOSS 1.002, TOP1 72.79, TOP5 93.27, TIME 90.995\n",
      "EPOCH 45, TLOSS 1.335, VLOSS 0.988, TOP1 73.66, TOP5 93.54, TIME 91.079\n",
      "EPOCH 46, TLOSS 1.336, VLOSS 1.018, TOP1 73.08, TOP5 93.58, TIME 90.912\n",
      "EPOCH 47, TLOSS 1.303, VLOSS 1.035, TOP1 73.28, TOP5 93.33, TIME 91.024\n",
      "EPOCH 48, TLOSS 1.306, VLOSS 0.951, TOP1 74.43, TOP5 93.82, TIME 90.868\n",
      "EPOCH 49, TLOSS 1.276, VLOSS 0.982, TOP1 73.61, TOP5 93.66, TIME 91.160\n",
      "EPOCH 50, TLOSS 1.275, VLOSS 0.984, TOP1 74.09, TOP5 93.82, TIME 90.906\n",
      "EPOCH 51, TLOSS 1.268, VLOSS 0.968, TOP1 74.55, TOP5 94.02, TIME 91.060\n",
      "EPOCH 52, TLOSS 1.264, VLOSS 0.993, TOP1 74.06, TOP5 94.06, TIME 91.163\n",
      "EPOCH 53, TLOSS 1.243, VLOSS 0.974, TOP1 74.33, TOP5 94.10, TIME 91.005\n",
      "EPOCH 54, TLOSS 1.201, VLOSS 0.988, TOP1 74.20, TOP5 94.07, TIME 90.817\n",
      "EPOCH 55, TLOSS 1.211, VLOSS 1.012, TOP1 74.46, TOP5 93.69, TIME 91.070\n",
      "EPOCH 56, TLOSS 1.181, VLOSS 0.994, TOP1 74.20, TOP5 93.88, TIME 91.212\n",
      "EPOCH 57, TLOSS 1.172, VLOSS 1.007, TOP1 74.30, TOP5 94.09, TIME 91.166\n",
      "EPOCH 58, TLOSS 1.132, VLOSS 1.030, TOP1 74.27, TOP5 93.75, TIME 91.160\n",
      "EPOCH 59, TLOSS 1.136, VLOSS 0.978, TOP1 74.79, TOP5 94.15, TIME 91.005\n",
      "EPOCH 60, TLOSS 1.154, VLOSS 0.973, TOP1 75.12, TOP5 94.38, TIME 91.051\n",
      "EPOCH 61, TLOSS 1.142, VLOSS 1.002, TOP1 75.07, TOP5 94.13, TIME 91.101\n",
      "EPOCH 62, TLOSS 1.130, VLOSS 1.000, TOP1 75.17, TOP5 94.00, TIME 90.919\n",
      "EPOCH 63, TLOSS 1.073, VLOSS 0.994, TOP1 74.86, TOP5 94.21, TIME 91.208\n",
      "EPOCH 64, TLOSS 1.081, VLOSS 0.991, TOP1 75.38, TOP5 94.10, TIME 91.092\n",
      "EPOCH 65, TLOSS 1.091, VLOSS 0.995, TOP1 76.06, TOP5 94.32, TIME 90.901\n",
      "EPOCH 66, TLOSS 1.073, VLOSS 1.026, TOP1 75.17, TOP5 94.06, TIME 91.016\n",
      "EPOCH 67, TLOSS 1.051, VLOSS 0.995, TOP1 75.38, TOP5 94.08, TIME 91.073\n",
      "EPOCH 68, TLOSS 1.050, VLOSS 1.029, TOP1 75.25, TOP5 93.88, TIME 90.842\n",
      "EPOCH 69, TLOSS 1.069, VLOSS 1.023, TOP1 75.88, TOP5 93.92, TIME 90.736\n",
      "EPOCH 70, TLOSS 1.058, VLOSS 1.012, TOP1 75.61, TOP5 94.05, TIME 90.914\n",
      "EPOCH 71, TLOSS 1.018, VLOSS 1.042, TOP1 75.05, TOP5 94.25, TIME 90.932\n",
      "EPOCH 72, TLOSS 1.010, VLOSS 1.025, TOP1 75.90, TOP5 94.00, TIME 90.906\n",
      "EPOCH 73, TLOSS 1.013, VLOSS 1.049, TOP1 75.79, TOP5 93.91, TIME 91.254\n",
      "EPOCH 74, TLOSS 0.981, VLOSS 1.008, TOP1 75.49, TOP5 94.11, TIME 91.045\n",
      "EPOCH 75, TLOSS 0.954, VLOSS 1.030, TOP1 75.97, TOP5 94.04, TIME 91.004\n",
      "EPOCH 76, TLOSS 0.979, VLOSS 0.997, TOP1 76.19, TOP5 94.06, TIME 90.960\n",
      "EPOCH 77, TLOSS 1.002, VLOSS 1.068, TOP1 75.42, TOP5 93.97, TIME 91.207\n",
      "EPOCH 78, TLOSS 0.977, VLOSS 1.034, TOP1 75.97, TOP5 93.89, TIME 90.964\n",
      "EPOCH 79, TLOSS 0.976, VLOSS 1.031, TOP1 76.08, TOP5 94.30, TIME 91.119\n",
      "EPOCH 80, TLOSS 0.970, VLOSS 1.043, TOP1 76.08, TOP5 94.40, TIME 91.074\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "EPOCH 81, TLOSS 0.976, VLOSS 1.080, TOP1 76.07, TOP5 94.08, TIME 91.052\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "EPOCH 82, TLOSS 0.872, VLOSS 0.960, TOP1 77.41, TOP5 94.76, TIME 90.992\n",
      "EPOCH 83, TLOSS 0.839, VLOSS 0.948, TOP1 77.50, TOP5 94.81, TIME 91.013\n",
      "EPOCH 84, TLOSS 0.818, VLOSS 0.936, TOP1 77.86, TOP5 94.92, TIME 90.907\n",
      "EPOCH 85, TLOSS 0.783, VLOSS 0.940, TOP1 77.96, TOP5 94.86, TIME 91.128\n",
      "EPOCH 86, TLOSS 0.807, VLOSS 0.934, TOP1 78.30, TOP5 94.86, TIME 91.023\n",
      "EPOCH 87, TLOSS 0.792, VLOSS 0.941, TOP1 78.16, TOP5 95.11, TIME 90.904\n",
      "EPOCH 88, TLOSS 0.787, VLOSS 0.922, TOP1 78.40, TOP5 95.10, TIME 91.156\n",
      "EPOCH 89, TLOSS 0.762, VLOSS 0.926, TOP1 78.32, TOP5 95.32, TIME 90.742\n",
      "EPOCH 90, TLOSS 0.743, VLOSS 0.901, TOP1 78.49, TOP5 95.09, TIME 90.896\n",
      "EPOCH 91, TLOSS 0.737, VLOSS 0.916, TOP1 78.75, TOP5 95.07, TIME 90.921\n",
      "EPOCH 92, TLOSS 0.754, VLOSS 0.927, TOP1 78.62, TOP5 95.11, TIME 91.080\n",
      "EPOCH 93, TLOSS 0.717, VLOSS 0.922, TOP1 78.59, TOP5 95.17, TIME 91.018\n",
      "EPOCH 94, TLOSS 0.731, VLOSS 0.946, TOP1 78.49, TOP5 95.03, TIME 90.978\n",
      "EPOCH 95, TLOSS 0.736, VLOSS 0.932, TOP1 78.77, TOP5 95.02, TIME 90.739\n",
      "EPOCH 96, TLOSS 0.731, VLOSS 0.934, TOP1 78.57, TOP5 95.04, TIME 90.922\n",
      "EPOCH 97, TLOSS 0.736, VLOSS 0.942, TOP1 78.65, TOP5 95.06, TIME 90.873\n",
      "EPOCH 98, TLOSS 0.733, VLOSS 0.941, TOP1 78.71, TOP5 95.09, TIME 90.943\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "EPOCH 99, TLOSS 0.740, VLOSS 0.941, TOP1 78.61, TOP5 95.17, TIME 90.811\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "EPOCH 100, TLOSS 0.738, VLOSS 0.937, TOP1 78.77, TOP5 95.19, TIME 90.800\n",
      "EPOCH 101, TLOSS 0.719, VLOSS 0.916, TOP1 78.85, TOP5 95.13, TIME 90.955\n",
      "EPOCH 102, TLOSS 0.701, VLOSS 0.924, TOP1 78.68, TOP5 95.14, TIME 91.049\n",
      "EPOCH 103, TLOSS 0.738, VLOSS 0.938, TOP1 78.91, TOP5 95.14, TIME 91.083\n",
      "EPOCH 104, TLOSS 0.729, VLOSS 0.929, TOP1 78.66, TOP5 95.03, TIME 90.898\n",
      "EPOCH 105, TLOSS 0.703, VLOSS 0.934, TOP1 78.79, TOP5 95.15, TIME 90.824\n",
      "EPOCH 106, TLOSS 0.714, VLOSS 0.928, TOP1 78.70, TOP5 95.04, TIME 90.834\n",
      "EPOCH 107, TLOSS 0.704, VLOSS 0.932, TOP1 78.87, TOP5 95.18, TIME 90.656\n",
      "EPOCH 108, TLOSS 0.696, VLOSS 0.930, TOP1 78.69, TOP5 95.09, TIME 90.883\n",
      "EPOCH 109, TLOSS 0.727, VLOSS 0.915, TOP1 79.09, TOP5 95.14, TIME 90.907\n",
      "EPOCH 110, TLOSS 0.717, VLOSS 0.929, TOP1 78.79, TOP5 95.15, TIME 90.775\n",
      "EPOCH 111, TLOSS 0.702, VLOSS 0.924, TOP1 78.77, TOP5 95.13, TIME 90.987\n",
      "EPOCH 112, TLOSS 0.717, VLOSS 0.933, TOP1 78.81, TOP5 95.22, TIME 91.033\n",
      "EPOCH 113, TLOSS 0.690, VLOSS 0.913, TOP1 78.66, TOP5 95.18, TIME 90.955\n",
      "EPOCH 114, TLOSS 0.729, VLOSS 0.930, TOP1 79.01, TOP5 95.13, TIME 90.847\n",
      "EPOCH 115, TLOSS 0.703, VLOSS 0.935, TOP1 78.98, TOP5 95.07, TIME 90.800\n",
      "EPOCH 116, TLOSS 0.694, VLOSS 0.933, TOP1 78.91, TOP5 95.16, TIME 90.760\n",
      "EPOCH 117, TLOSS 0.715, VLOSS 0.930, TOP1 78.83, TOP5 95.18, TIME 90.931\n",
      "EPOCH 118, TLOSS 0.728, VLOSS 0.921, TOP1 78.93, TOP5 95.09, TIME 91.156\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "EPOCH 119, TLOSS 0.713, VLOSS 0.903, TOP1 78.98, TOP5 95.17, TIME 90.983\n",
      "EPOCH 120, TLOSS 0.722, VLOSS 0.943, TOP1 78.93, TOP5 95.10, TIME 90.892\n",
      "EPOCH 121, TLOSS 0.723, VLOSS 0.930, TOP1 79.08, TOP5 95.22, TIME 90.960\n",
      "EPOCH 122, TLOSS 0.692, VLOSS 0.937, TOP1 78.75, TOP5 95.11, TIME 90.960\n",
      "EPOCH 123, TLOSS 0.718, VLOSS 0.925, TOP1 79.04, TOP5 95.05, TIME 91.070\n",
      "EPOCH 124, TLOSS 0.701, VLOSS 0.921, TOP1 78.94, TOP5 95.22, TIME 90.830\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 125, TLOSS 0.714, VLOSS 0.927, TOP1 78.93, TOP5 95.10, TIME 91.019\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 126, TLOSS 0.709, VLOSS 0.932, TOP1 78.84, TOP5 95.13, TIME 90.995\n",
      "EPOCH 127, TLOSS 0.727, VLOSS 0.923, TOP1 78.86, TOP5 95.12, TIME 90.935\n",
      "EPOCH 128, TLOSS 0.690, VLOSS 0.922, TOP1 78.97, TOP5 95.18, TIME 91.104\n",
      "EPOCH 129, TLOSS 0.726, VLOSS 0.927, TOP1 78.91, TOP5 95.07, TIME 90.944\n",
      "EPOCH 130, TLOSS 0.694, VLOSS 0.926, TOP1 78.90, TOP5 95.21, TIME 90.841\n",
      "EPOCH 131, TLOSS 0.699, VLOSS 0.910, TOP1 78.84, TOP5 95.14, TIME 90.703\n",
      "EPOCH 132, TLOSS 0.683, VLOSS 0.930, TOP1 78.95, TOP5 95.23, TIME 90.728\n",
      "EPOCH 133, TLOSS 0.707, VLOSS 0.923, TOP1 79.01, TOP5 95.11, TIME 90.919\n",
      "EPOCH 134, TLOSS 0.711, VLOSS 0.950, TOP1 78.88, TOP5 95.03, TIME 90.900\n",
      "EPOCH 135, TLOSS 0.704, VLOSS 0.914, TOP1 78.79, TOP5 95.11, TIME 90.943\n",
      "EPOCH 136, TLOSS 0.711, VLOSS 0.930, TOP1 78.79, TOP5 95.10, TIME 90.806\n",
      "EPOCH 137, TLOSS 0.705, VLOSS 0.934, TOP1 78.91, TOP5 95.16, TIME 90.699\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-08.\n",
      "EPOCH 138, TLOSS 0.701, VLOSS 0.916, TOP1 78.92, TOP5 95.08, TIME 90.837\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-08.\n",
      "EPOCH 139, TLOSS 0.697, VLOSS 0.923, TOP1 78.73, TOP5 95.16, TIME 90.724\n",
      "EPOCH 140, TLOSS 0.695, VLOSS 0.927, TOP1 78.83, TOP5 95.18, TIME 90.759\n",
      "EPOCH 141, TLOSS 0.720, VLOSS 0.936, TOP1 78.83, TOP5 95.13, TIME 90.737\n",
      "EPOCH 142, TLOSS 0.692, VLOSS 0.927, TOP1 78.93, TOP5 95.03, TIME 90.627\n",
      "EPOCH 143, TLOSS 0.691, VLOSS 0.918, TOP1 78.83, TOP5 95.06, TIME 90.816\n",
      "EPOCH 144, TLOSS 0.692, VLOSS 0.916, TOP1 78.85, TOP5 95.04, TIME 90.744\n",
      "EPOCH 145, TLOSS 0.695, VLOSS 0.921, TOP1 79.04, TOP5 95.07, TIME 90.867\n",
      "EPOCH 146, TLOSS 0.701, VLOSS 0.922, TOP1 78.97, TOP5 95.10, TIME 90.812\n",
      "EPOCH 147, TLOSS 0.688, VLOSS 0.925, TOP1 78.89, TOP5 95.02, TIME 91.013\n",
      "EPOCH 148, TLOSS 0.714, VLOSS 0.916, TOP1 78.72, TOP5 95.14, TIME 90.898\n",
      "EPOCH 149, TLOSS 0.723, VLOSS 0.937, TOP1 78.96, TOP5 95.05, TIME 90.799\n",
      "EPOCH 150, TLOSS 0.694, VLOSS 0.924, TOP1 78.92, TOP5 95.11, TIME 90.880\n",
      "EPOCH 151, TLOSS 0.717, VLOSS 0.942, TOP1 78.91, TOP5 95.10, TIME 90.666\n",
      "EPOCH 152, TLOSS 0.689, VLOSS 0.923, TOP1 78.82, TOP5 95.13, TIME 90.652\n",
      "EPOCH 153, TLOSS 0.705, VLOSS 0.927, TOP1 78.72, TOP5 95.03, TIME 90.772\n",
      "EPOCH 154, TLOSS 0.703, VLOSS 0.927, TOP1 79.05, TOP5 95.05, TIME 90.650\n",
      "EPOCH 155, TLOSS 0.697, VLOSS 0.929, TOP1 78.98, TOP5 95.12, TIME 90.631\n",
      "EPOCH 156, TLOSS 0.705, VLOSS 0.924, TOP1 78.94, TOP5 95.19, TIME 90.592\n",
      "EPOCH 157, TLOSS 0.692, VLOSS 0.918, TOP1 78.85, TOP5 95.06, TIME 90.724\n",
      "EPOCH 158, TLOSS 0.686, VLOSS 0.919, TOP1 78.97, TOP5 94.99, TIME 90.714\n",
      "EPOCH 159, TLOSS 0.703, VLOSS 0.918, TOP1 78.83, TOP5 94.99, TIME 90.659\n",
      "EPOCH 160, TLOSS 0.710, VLOSS 0.919, TOP1 78.88, TOP5 95.12, TIME 90.743\n",
      "EPOCH 161, TLOSS 0.703, VLOSS 0.917, TOP1 78.90, TOP5 95.04, TIME 90.740\n",
      "EPOCH 162, TLOSS 0.722, VLOSS 0.931, TOP1 79.06, TOP5 95.10, TIME 90.660\n",
      "EPOCH 163, TLOSS 0.713, VLOSS 0.930, TOP1 78.81, TOP5 95.05, TIME 90.602\n",
      "EPOCH 164, TLOSS 0.704, VLOSS 0.940, TOP1 78.82, TOP5 95.02, TIME 90.747\n",
      "EPOCH 165, TLOSS 0.707, VLOSS 0.909, TOP1 78.81, TOP5 95.07, TIME 90.621\n",
      "EPOCH 166, TLOSS 0.704, VLOSS 0.932, TOP1 78.86, TOP5 95.18, TIME 90.711\n",
      "EPOCH 167, TLOSS 0.715, VLOSS 0.914, TOP1 78.94, TOP5 95.06, TIME 90.590\n",
      "EPOCH 168, TLOSS 0.683, VLOSS 0.928, TOP1 78.99, TOP5 95.17, TIME 90.642\n",
      "EPOCH 169, TLOSS 0.681, VLOSS 0.917, TOP1 78.90, TOP5 95.20, TIME 90.527\n",
      "EPOCH 170, TLOSS 0.707, VLOSS 0.937, TOP1 79.02, TOP5 95.05, TIME 90.567\n",
      "EPOCH 171, TLOSS 0.684, VLOSS 0.922, TOP1 79.04, TOP5 95.12, TIME 90.416\n",
      "EPOCH 172, TLOSS 0.696, VLOSS 0.939, TOP1 78.81, TOP5 95.08, TIME 90.606\n",
      "EPOCH 173, TLOSS 0.690, VLOSS 0.927, TOP1 78.80, TOP5 95.12, TIME 90.690\n",
      "EPOCH 174, TLOSS 0.711, VLOSS 0.939, TOP1 78.81, TOP5 95.03, TIME 90.787\n",
      "EPOCH 175, TLOSS 0.708, VLOSS 0.944, TOP1 79.01, TOP5 95.07, TIME 90.557\n",
      "EPOCH 176, TLOSS 0.677, VLOSS 0.918, TOP1 78.79, TOP5 95.09, TIME 90.817\n",
      "EPOCH 177, TLOSS 0.684, VLOSS 0.922, TOP1 78.83, TOP5 95.14, TIME 90.718\n",
      "EPOCH 178, TLOSS 0.719, VLOSS 0.931, TOP1 78.91, TOP5 95.16, TIME 90.598\n",
      "EPOCH 179, TLOSS 0.706, VLOSS 0.930, TOP1 78.97, TOP5 95.02, TIME 90.573\n",
      "EPOCH 180, TLOSS 0.711, VLOSS 0.927, TOP1 78.88, TOP5 95.10, TIME 90.495\n",
      "EPOCH 181, TLOSS 0.704, VLOSS 0.927, TOP1 78.87, TOP5 95.05, TIME 90.727\n",
      "EPOCH 182, TLOSS 0.682, VLOSS 0.924, TOP1 78.98, TOP5 95.19, TIME 90.636\n",
      "EPOCH 183, TLOSS 0.711, VLOSS 0.928, TOP1 78.77, TOP5 95.09, TIME 90.806\n",
      "EPOCH 184, TLOSS 0.702, VLOSS 0.919, TOP1 78.91, TOP5 95.20, TIME 90.822\n",
      "EPOCH 185, TLOSS 0.729, VLOSS 0.919, TOP1 79.01, TOP5 95.15, TIME 90.712\n",
      "EPOCH 186, TLOSS 0.718, VLOSS 0.939, TOP1 78.94, TOP5 95.02, TIME 90.702\n",
      "EPOCH 187, TLOSS 0.713, VLOSS 0.937, TOP1 78.97, TOP5 95.11, TIME 90.745\n",
      "EPOCH 188, TLOSS 0.707, VLOSS 0.922, TOP1 78.84, TOP5 95.08, TIME 90.798\n",
      "EPOCH 189, TLOSS 0.690, VLOSS 0.917, TOP1 78.73, TOP5 95.10, TIME 90.741\n",
      "EPOCH 190, TLOSS 0.711, VLOSS 0.925, TOP1 78.93, TOP5 95.19, TIME 90.848\n",
      "EPOCH 191, TLOSS 0.693, VLOSS 0.922, TOP1 78.96, TOP5 95.12, TIME 90.578\n",
      "EPOCH 192, TLOSS 0.715, VLOSS 0.941, TOP1 79.10, TOP5 95.07, TIME 90.779\n",
      "EPOCH 193, TLOSS 0.699, VLOSS 0.928, TOP1 78.83, TOP5 95.15, TIME 90.703\n",
      "EPOCH 194, TLOSS 0.723, VLOSS 0.925, TOP1 78.89, TOP5 95.02, TIME 90.729\n",
      "EPOCH 195, TLOSS 0.709, VLOSS 0.922, TOP1 78.89, TOP5 95.00, TIME 90.497\n",
      "EPOCH 196, TLOSS 0.696, VLOSS 0.919, TOP1 78.99, TOP5 95.21, TIME 90.841\n",
      "EPOCH 197, TLOSS 0.717, VLOSS 0.914, TOP1 78.80, TOP5 95.12, TIME 90.739\n",
      "EPOCH 198, TLOSS 0.719, VLOSS 0.930, TOP1 78.74, TOP5 95.14, TIME 90.898\n",
      "EPOCH 199, TLOSS 0.697, VLOSS 0.929, TOP1 79.06, TOP5 95.18, TIME 90.827\n",
      "EPOCH 200, TLOSS 0.702, VLOSS 0.932, TOP1 78.97, TOP5 94.97, TIME 90.689\n",
      "EPOCH 201, TLOSS 0.713, VLOSS 0.912, TOP1 78.77, TOP5 95.09, TIME 91.001\n",
      "EPOCH 202, TLOSS 0.697, VLOSS 0.926, TOP1 79.08, TOP5 95.13, TIME 90.858\n",
      "EPOCH 203, TLOSS 0.714, VLOSS 0.938, TOP1 79.03, TOP5 95.05, TIME 90.944\n",
      "EPOCH 204, TLOSS 0.710, VLOSS 0.934, TOP1 79.11, TOP5 95.01, TIME 90.618\n",
      "EPOCH 205, TLOSS 0.692, VLOSS 0.929, TOP1 78.83, TOP5 95.06, TIME 90.688\n",
      "EPOCH 206, TLOSS 0.695, VLOSS 0.922, TOP1 78.93, TOP5 95.12, TIME 90.673\n",
      "EPOCH 207, TLOSS 0.706, VLOSS 0.924, TOP1 78.99, TOP5 95.10, TIME 90.759\n",
      "EPOCH 208, TLOSS 0.685, VLOSS 0.929, TOP1 78.87, TOP5 95.06, TIME 90.816\n",
      "EPOCH 209, TLOSS 0.702, VLOSS 0.916, TOP1 78.92, TOP5 95.13, TIME 90.748\n",
      "EPOCH 210, TLOSS 0.698, VLOSS 0.936, TOP1 78.92, TOP5 95.11, TIME 90.745\n",
      "EPOCH 211, TLOSS 0.710, VLOSS 0.922, TOP1 78.81, TOP5 95.15, TIME 90.739\n",
      "EPOCH 212, TLOSS 0.707, VLOSS 0.929, TOP1 78.94, TOP5 95.12, TIME 90.777\n",
      "EPOCH 213, TLOSS 0.719, VLOSS 0.929, TOP1 78.82, TOP5 95.11, TIME 90.718\n",
      "EPOCH 214, TLOSS 0.699, VLOSS 0.935, TOP1 78.99, TOP5 95.06, TIME 90.778\n",
      "EPOCH 215, TLOSS 0.706, VLOSS 0.944, TOP1 79.03, TOP5 95.08, TIME 90.624\n",
      "EPOCH 216, TLOSS 0.709, VLOSS 0.922, TOP1 78.84, TOP5 95.14, TIME 90.841\n",
      "EPOCH 217, TLOSS 0.717, VLOSS 0.919, TOP1 78.92, TOP5 95.04, TIME 90.771\n",
      "EPOCH 218, TLOSS 0.695, VLOSS 0.919, TOP1 78.93, TOP5 95.11, TIME 90.858\n",
      "EPOCH 219, TLOSS 0.717, VLOSS 0.933, TOP1 78.92, TOP5 95.22, TIME 90.992\n",
      "EPOCH 220, TLOSS 0.700, VLOSS 0.914, TOP1 78.89, TOP5 95.17, TIME 90.715\n",
      "EPOCH 221, TLOSS 0.705, VLOSS 0.923, TOP1 78.88, TOP5 95.16, TIME 90.811\n",
      "EPOCH 222, TLOSS 0.689, VLOSS 0.915, TOP1 78.96, TOP5 95.11, TIME 90.838\n",
      "EPOCH 223, TLOSS 0.716, VLOSS 0.918, TOP1 78.97, TOP5 95.15, TIME 90.984\n",
      "EPOCH 224, TLOSS 0.697, VLOSS 0.934, TOP1 78.92, TOP5 95.17, TIME 90.903\n",
      "EPOCH 225, TLOSS 0.715, VLOSS 0.924, TOP1 78.93, TOP5 95.07, TIME 90.885\n",
      "EPOCH 226, TLOSS 0.715, VLOSS 0.947, TOP1 79.01, TOP5 95.11, TIME 90.964\n",
      "EPOCH 227, TLOSS 0.704, VLOSS 0.934, TOP1 78.88, TOP5 95.14, TIME 90.833\n",
      "EPOCH 228, TLOSS 0.696, VLOSS 0.927, TOP1 78.96, TOP5 95.19, TIME 90.783\n",
      "EPOCH 229, TLOSS 0.686, VLOSS 0.925, TOP1 78.97, TOP5 95.19, TIME 90.826\n",
      "EPOCH 230, TLOSS 0.709, VLOSS 0.915, TOP1 78.85, TOP5 95.17, TIME 90.983\n",
      "EPOCH 231, TLOSS 0.715, VLOSS 0.914, TOP1 78.87, TOP5 95.08, TIME 90.736\n",
      "EPOCH 232, TLOSS 0.732, VLOSS 0.926, TOP1 79.00, TOP5 95.11, TIME 90.975\n",
      "EPOCH 233, TLOSS 0.714, VLOSS 0.923, TOP1 78.69, TOP5 95.15, TIME 90.921\n",
      "EPOCH 234, TLOSS 0.703, VLOSS 0.917, TOP1 78.90, TOP5 95.10, TIME 90.833\n",
      "EPOCH 235, TLOSS 0.707, VLOSS 0.955, TOP1 78.88, TOP5 95.01, TIME 90.972\n",
      "EPOCH 236, TLOSS 0.723, VLOSS 0.928, TOP1 79.03, TOP5 95.13, TIME 90.602\n",
      "EPOCH 237, TLOSS 0.708, VLOSS 0.937, TOP1 78.91, TOP5 95.03, TIME 90.715\n",
      "EPOCH 238, TLOSS 0.708, VLOSS 0.929, TOP1 79.04, TOP5 95.07, TIME 90.786\n",
      "EPOCH 239, TLOSS 0.712, VLOSS 0.938, TOP1 78.93, TOP5 95.03, TIME 90.465\n",
      "EPOCH 240, TLOSS 0.695, VLOSS 0.934, TOP1 78.82, TOP5 95.04, TIME 90.542\n",
      "EPOCH 241, TLOSS 0.723, VLOSS 0.933, TOP1 78.98, TOP5 95.15, TIME 90.769\n",
      "EPOCH 242, TLOSS 0.693, VLOSS 0.927, TOP1 78.94, TOP5 95.12, TIME 90.665\n",
      "EPOCH 243, TLOSS 0.695, VLOSS 0.913, TOP1 78.87, TOP5 95.15, TIME 90.685\n",
      "EPOCH 244, TLOSS 0.694, VLOSS 0.921, TOP1 79.00, TOP5 95.14, TIME 90.647\n",
      "EPOCH 245, TLOSS 0.722, VLOSS 0.931, TOP1 78.90, TOP5 95.10, TIME 90.776\n",
      "EPOCH 246, TLOSS 0.710, VLOSS 0.938, TOP1 79.05, TOP5 95.16, TIME 90.849\n",
      "EPOCH 247, TLOSS 0.709, VLOSS 0.934, TOP1 78.95, TOP5 95.11, TIME 90.674\n",
      "EPOCH 248, TLOSS 0.688, VLOSS 0.920, TOP1 78.93, TOP5 95.22, TIME 90.895\n",
      "EPOCH 249, TLOSS 0.701, VLOSS 0.918, TOP1 78.92, TOP5 95.13, TIME 90.856\n"
     ]
    }
   ],
   "source": [
    "# DDP training routine inputs\n",
    "model_type = 'effnetv2'\n",
    "world_size = 6\n",
    "time_budget_mins = np.inf # minutes per trial\n",
    "nepochs = 250\n",
    "batch_size = 25\n",
    "accumulate = 4\n",
    "evaluate = True\n",
    "saving = 'final'\n",
    "\n",
    "# Import or define model parameters\n",
    "model_parameters = {\"output_size\": 100}\n",
    "\n",
    "%run -i bte_ddp.py\n",
    "\n",
    "with open(f'experiment_results/{model_type}_result.pkl', 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "    \n",
    "# Unpack results object\n",
    "avg_epoch_train_time = [results[e][\"time\"] for e in results]\n",
    "train_loss_epoch = np.array([results[e][\"tloss\"] for e in results])\n",
    "valid_loss_epoch = np.array([results[e][\"vloss\"] for e in results])\n",
    "valid_top1accu_epoch = np.array([results[e][\"top1\"] for e in results])\n",
    "valid_top5accu_epoch = np.array([results[e][\"top5\"] for e in results])\n",
    "\n",
    "target = valid_top1accu_epoch.max()\n",
    "\n",
    "# Save trial result measures to disk for inspection later.\n",
    "payload = {\n",
    "    'R': R,\n",
    "    'time_budget_mins': time_budget_mins,\n",
    "    'params': model_parameters,\n",
    "    'tloss':train_loss_epoch,\n",
    "    'vloss':valid_loss_epoch,\n",
    "    'top1': valid_top1accu_epoch,\n",
    "    'top5': valid_top5accu_epoch,\n",
    "    'time': avg_epoch_train_time\n",
    "}\n",
    "\n",
    "with open(f'experiment_results/EffNetV2_results.pkl', 'wb') as handle:\n",
    "    pickle.dump(payload, handle)\n",
    "\n",
    "# Need to delete this because otherwise failed training is skipped and the file from last run is picked up instead.\n",
    "os.remove(f'experiment_results/{model_type}_result.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a2f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bdf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
