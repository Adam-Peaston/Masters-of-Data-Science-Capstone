{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84bcb04",
   "metadata": {},
   "source": [
    "## 1. Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff78d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, math, time, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7df8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models, datasets, transforms\n",
    "from components import ToRGB, BetaNet2, BetaNet3, EffNetV1, EffNetV2, MobileNetV2, ResNet50\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f90ea",
   "metadata": {},
   "source": [
    "## 2. Training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b46873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train: 50,000, Valid: 10,000\n"
     ]
    }
   ],
   "source": [
    "# Set up train and validation datasets\n",
    "norm_stats = ((0.5071, 0.4866, 0.4409),(0.2009, 0.1984, 0.2023)) # CIFAR100 training set normalization constants\n",
    "R = 384\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.AutoAugment(policy = transforms.autoaugment.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(R),\n",
    "    transforms.ToTensor(), # Also standardizes to range [0,1]\n",
    "    transforms.Normalize(*norm_stats),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(R),\n",
    "    transforms.ToTensor(), # Also standardizes to range [0,1]\n",
    "    transforms.Normalize(*norm_stats),\n",
    "])\n",
    "\n",
    "ROOT = '/data/cifar100'\n",
    "train_dataset = datasets.CIFAR100(root=ROOT, train=True, transform=train_transform, download=True)\n",
    "\n",
    "# Hold-out this data for final evaluation\n",
    "valid_dataset = datasets.CIFAR100(root=ROOT, train=False, transform=valid_transform, download=True)\n",
    "\n",
    "print(f'Train: {len(train_dataset):,.0f}, Valid: {len(valid_dataset):,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6e015",
   "metadata": {},
   "source": [
    "## 3. Run training and evaluation routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2a2de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated beta2 model with 30,011,085 params.\n",
      "EPOCH 0, TLOSS 4.398, VLOSS 3.976, TOP1 7.73, TOP5 26.86, TIME 54.955\n",
      "EPOCH 1, TLOSS 4.061, VLOSS 3.586, TOP1 13.86, TOP5 38.67, TIME 52.807\n",
      "EPOCH 2, TLOSS 3.771, VLOSS 3.170, TOP1 20.97, TOP5 50.94, TIME 52.994\n",
      "EPOCH 3, TLOSS 3.498, VLOSS 2.769, TOP1 28.86, TOP5 61.04, TIME 53.062\n",
      "EPOCH 4, TLOSS 3.271, VLOSS 2.482, TOP1 34.86, TOP5 67.28, TIME 53.082\n",
      "EPOCH 5, TLOSS 3.069, VLOSS 2.314, TOP1 38.38, TOP5 71.11, TIME 53.099\n",
      "EPOCH 6, TLOSS 2.904, VLOSS 2.141, TOP1 42.40, TOP5 74.54, TIME 53.275\n",
      "EPOCH 7, TLOSS 2.761, VLOSS 2.016, TOP1 45.31, TOP5 76.91, TIME 53.222\n",
      "EPOCH 8, TLOSS 2.662, VLOSS 1.912, TOP1 48.50, TOP5 79.11, TIME 53.202\n",
      "EPOCH 9, TLOSS 2.580, VLOSS 1.865, TOP1 49.08, TOP5 79.44, TIME 53.164\n",
      "EPOCH 10, TLOSS 2.473, VLOSS 1.663, TOP1 53.82, TOP5 82.90, TIME 53.077\n",
      "EPOCH 11, TLOSS 2.403, VLOSS 1.602, TOP1 55.22, TOP5 84.24, TIME 53.121\n",
      "EPOCH 12, TLOSS 2.318, VLOSS 1.575, TOP1 56.04, TOP5 84.61, TIME 53.236\n",
      "EPOCH 13, TLOSS 2.281, VLOSS 1.439, TOP1 58.79, TOP5 87.23, TIME 52.991\n",
      "EPOCH 14, TLOSS 2.220, VLOSS 1.527, TOP1 57.16, TOP5 85.76, TIME 53.119\n",
      "EPOCH 15, TLOSS 2.173, VLOSS 1.408, TOP1 59.92, TOP5 87.66, TIME 52.953\n",
      "EPOCH 16, TLOSS 2.102, VLOSS 1.407, TOP1 60.62, TOP5 87.37, TIME 53.122\n",
      "EPOCH 17, TLOSS 2.059, VLOSS 1.373, TOP1 60.21, TOP5 87.88, TIME 52.953\n",
      "EPOCH 18, TLOSS 2.021, VLOSS 1.292, TOP1 63.09, TOP5 89.03, TIME 53.351\n",
      "EPOCH 19, TLOSS 1.997, VLOSS 1.294, TOP1 62.85, TOP5 88.90, TIME 53.095\n",
      "EPOCH 20, TLOSS 1.933, VLOSS 1.239, TOP1 63.95, TOP5 89.95, TIME 53.034\n",
      "EPOCH 21, TLOSS 1.912, VLOSS 1.206, TOP1 65.16, TOP5 90.26, TIME 53.092\n",
      "EPOCH 22, TLOSS 1.905, VLOSS 1.199, TOP1 65.82, TOP5 90.13, TIME 53.173\n",
      "EPOCH 23, TLOSS 1.838, VLOSS 1.206, TOP1 65.74, TOP5 90.23, TIME 53.204\n",
      "EPOCH 24, TLOSS 1.828, VLOSS 1.200, TOP1 66.00, TOP5 90.68, TIME 53.107\n",
      "EPOCH 25, TLOSS 1.796, VLOSS 1.119, TOP1 67.75, TOP5 91.24, TIME 53.399\n",
      "EPOCH 26, TLOSS 1.765, VLOSS 1.125, TOP1 67.77, TOP5 91.35, TIME 53.231\n",
      "EPOCH 27, TLOSS 1.725, VLOSS 1.149, TOP1 67.67, TOP5 91.60, TIME 53.078\n",
      "EPOCH 28, TLOSS 1.725, VLOSS 1.088, TOP1 69.00, TOP5 91.52, TIME 53.033\n",
      "EPOCH 29, TLOSS 1.685, VLOSS 1.123, TOP1 68.40, TOP5 91.67, TIME 52.890\n",
      "EPOCH 30, TLOSS 1.686, VLOSS 1.066, TOP1 69.55, TOP5 92.01, TIME 52.944\n",
      "EPOCH 31, TLOSS 1.635, VLOSS 1.052, TOP1 70.33, TOP5 92.34, TIME 53.081\n",
      "EPOCH 32, TLOSS 1.621, VLOSS 1.048, TOP1 69.96, TOP5 92.27, TIME 52.957\n",
      "EPOCH 33, TLOSS 1.588, VLOSS 1.061, TOP1 69.56, TOP5 92.46, TIME 53.226\n",
      "EPOCH 34, TLOSS 1.587, VLOSS 1.041, TOP1 70.57, TOP5 92.60, TIME 52.957\n",
      "EPOCH 35, TLOSS 1.536, VLOSS 1.044, TOP1 70.69, TOP5 92.45, TIME 53.049\n",
      "EPOCH 36, TLOSS 1.518, VLOSS 0.995, TOP1 71.53, TOP5 92.93, TIME 52.973\n",
      "EPOCH 37, TLOSS 1.528, VLOSS 1.027, TOP1 70.95, TOP5 92.68, TIME 53.068\n",
      "EPOCH 38, TLOSS 1.495, VLOSS 1.020, TOP1 70.92, TOP5 92.65, TIME 52.863\n",
      "EPOCH 39, TLOSS 1.473, VLOSS 1.010, TOP1 72.00, TOP5 92.73, TIME 52.927\n",
      "EPOCH 40, TLOSS 1.458, VLOSS 1.016, TOP1 71.91, TOP5 92.96, TIME 52.941\n",
      "EPOCH 41, TLOSS 1.424, VLOSS 0.985, TOP1 72.54, TOP5 92.92, TIME 53.301\n",
      "EPOCH 42, TLOSS 1.427, VLOSS 0.993, TOP1 72.36, TOP5 93.05, TIME 53.022\n",
      "EPOCH 43, TLOSS 1.393, VLOSS 0.990, TOP1 72.35, TOP5 93.06, TIME 53.089\n",
      "EPOCH 44, TLOSS 1.407, VLOSS 0.981, TOP1 73.13, TOP5 93.17, TIME 53.056\n",
      "EPOCH 45, TLOSS 1.366, VLOSS 1.035, TOP1 72.43, TOP5 93.07, TIME 53.210\n",
      "EPOCH 46, TLOSS 1.366, VLOSS 1.007, TOP1 72.93, TOP5 93.01, TIME 53.067\n",
      "EPOCH 47, TLOSS 1.334, VLOSS 1.010, TOP1 72.58, TOP5 92.86, TIME 52.967\n",
      "EPOCH 48, TLOSS 1.335, VLOSS 1.020, TOP1 73.03, TOP5 93.29, TIME 53.069\n",
      "EPOCH 49, TLOSS 1.309, VLOSS 0.982, TOP1 74.17, TOP5 93.57, TIME 53.084\n",
      "EPOCH 50, TLOSS 1.305, VLOSS 1.007, TOP1 72.72, TOP5 93.36, TIME 52.917\n",
      "EPOCH 51, TLOSS 1.316, VLOSS 0.993, TOP1 73.38, TOP5 93.57, TIME 53.067\n",
      "EPOCH 52, TLOSS 1.310, VLOSS 1.002, TOP1 73.85, TOP5 93.38, TIME 52.797\n",
      "EPOCH 53, TLOSS 1.289, VLOSS 0.992, TOP1 73.56, TOP5 93.64, TIME 53.029\n",
      "EPOCH 54, TLOSS 1.243, VLOSS 1.006, TOP1 73.62, TOP5 93.55, TIME 53.117\n",
      "EPOCH 55, TLOSS 1.246, VLOSS 1.004, TOP1 73.68, TOP5 93.32, TIME 52.785\n",
      "EPOCH 56, TLOSS 1.238, VLOSS 0.997, TOP1 74.23, TOP5 93.53, TIME 53.070\n",
      "EPOCH 57, TLOSS 1.230, VLOSS 1.038, TOP1 73.56, TOP5 93.47, TIME 53.219\n",
      "EPOCH 58, TLOSS 1.199, VLOSS 0.997, TOP1 74.67, TOP5 93.77, TIME 53.284\n",
      "EPOCH 59, TLOSS 1.189, VLOSS 0.994, TOP1 74.14, TOP5 93.90, TIME 53.152\n",
      "EPOCH 60, TLOSS 1.190, VLOSS 1.024, TOP1 74.36, TOP5 93.66, TIME 53.031\n",
      "EPOCH 61, TLOSS 1.190, VLOSS 1.021, TOP1 74.26, TOP5 93.67, TIME 52.945\n",
      "EPOCH 62, TLOSS 1.164, VLOSS 1.015, TOP1 73.97, TOP5 93.78, TIME 53.309\n",
      "EPOCH 63, TLOSS 1.141, VLOSS 1.013, TOP1 74.29, TOP5 93.95, TIME 53.267\n",
      "EPOCH 64, TLOSS 1.147, VLOSS 1.013, TOP1 74.52, TOP5 93.76, TIME 53.049\n",
      "EPOCH 65, TLOSS 1.155, VLOSS 0.992, TOP1 74.86, TOP5 93.82, TIME 52.957\n",
      "EPOCH 66, TLOSS 1.140, VLOSS 1.011, TOP1 74.51, TOP5 93.88, TIME 53.115\n",
      "EPOCH 67, TLOSS 1.108, VLOSS 1.008, TOP1 74.96, TOP5 93.95, TIME 53.292\n",
      "EPOCH 68, TLOSS 1.099, VLOSS 1.014, TOP1 74.76, TOP5 93.68, TIME 53.105\n",
      "EPOCH 69, TLOSS 1.118, VLOSS 1.029, TOP1 74.50, TOP5 93.94, TIME 53.343\n",
      "EPOCH 70, TLOSS 1.116, VLOSS 1.015, TOP1 74.96, TOP5 93.90, TIME 53.092\n",
      "EPOCH 71, TLOSS 1.079, VLOSS 1.033, TOP1 74.80, TOP5 93.82, TIME 53.230\n",
      "EPOCH 72, TLOSS 1.080, VLOSS 1.041, TOP1 74.77, TOP5 93.82, TIME 53.204\n",
      "EPOCH 73, TLOSS 1.085, VLOSS 1.031, TOP1 74.82, TOP5 93.87, TIME 53.261\n",
      "EPOCH 74, TLOSS 1.045, VLOSS 1.022, TOP1 74.88, TOP5 93.59, TIME 53.028\n",
      "EPOCH 75, TLOSS 1.023, VLOSS 1.007, TOP1 75.00, TOP5 93.66, TIME 53.323\n",
      "EPOCH 76, TLOSS 1.046, VLOSS 1.012, TOP1 74.71, TOP5 93.70, TIME 53.209\n",
      "EPOCH 77, TLOSS 1.056, VLOSS 1.011, TOP1 75.58, TOP5 93.99, TIME 53.085\n",
      "EPOCH 78, TLOSS 1.029, VLOSS 1.035, TOP1 75.27, TOP5 93.83, TIME 52.916\n",
      "EPOCH 79, TLOSS 1.035, VLOSS 1.012, TOP1 75.16, TOP5 93.89, TIME 53.192\n",
      "EPOCH 80, TLOSS 1.028, VLOSS 1.034, TOP1 74.92, TOP5 93.87, TIME 53.275\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "EPOCH 81, TLOSS 1.031, VLOSS 1.050, TOP1 75.36, TOP5 93.56, TIME 53.183\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "EPOCH 82, TLOSS 0.939, VLOSS 0.948, TOP1 76.96, TOP5 94.43, TIME 53.253\n",
      "EPOCH 83, TLOSS 0.913, VLOSS 0.931, TOP1 77.32, TOP5 94.56, TIME 53.207\n",
      "EPOCH 84, TLOSS 0.900, VLOSS 0.925, TOP1 77.20, TOP5 94.67, TIME 53.257\n",
      "EPOCH 85, TLOSS 0.856, VLOSS 0.919, TOP1 77.52, TOP5 94.73, TIME 53.155\n",
      "EPOCH 86, TLOSS 0.883, VLOSS 0.914, TOP1 77.56, TOP5 94.77, TIME 53.081\n",
      "EPOCH 87, TLOSS 0.869, VLOSS 0.911, TOP1 77.82, TOP5 94.89, TIME 53.160\n",
      "EPOCH 88, TLOSS 0.859, VLOSS 0.915, TOP1 77.84, TOP5 94.78, TIME 53.031\n",
      "EPOCH 89, TLOSS 0.844, VLOSS 0.912, TOP1 77.86, TOP5 94.74, TIME 53.103\n",
      "EPOCH 90, TLOSS 0.828, VLOSS 0.917, TOP1 77.82, TOP5 94.79, TIME 53.181\n",
      "EPOCH 91, TLOSS 0.815, VLOSS 0.911, TOP1 78.08, TOP5 94.81, TIME 53.047\n",
      "EPOCH 92, TLOSS 0.838, VLOSS 0.921, TOP1 77.72, TOP5 94.79, TIME 53.289\n",
      "EPOCH 93, TLOSS 0.803, VLOSS 0.914, TOP1 77.92, TOP5 94.83, TIME 53.152\n",
      "EPOCH 94, TLOSS 0.808, VLOSS 0.925, TOP1 78.15, TOP5 94.80, TIME 53.040\n",
      "EPOCH 95, TLOSS 0.821, VLOSS 0.928, TOP1 77.95, TOP5 94.90, TIME 53.463\n",
      "EPOCH 96, TLOSS 0.810, VLOSS 0.922, TOP1 77.99, TOP5 95.00, TIME 53.271\n",
      "EPOCH 97, TLOSS 0.804, VLOSS 0.922, TOP1 78.37, TOP5 94.91, TIME 53.286\n",
      "EPOCH 98, TLOSS 0.806, VLOSS 0.924, TOP1 77.99, TOP5 94.79, TIME 53.387\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "EPOCH 99, TLOSS 0.806, VLOSS 0.926, TOP1 78.33, TOP5 94.96, TIME 53.231\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "EPOCH 100, TLOSS 0.822, VLOSS 0.923, TOP1 78.28, TOP5 94.87, TIME 53.263\n",
      "EPOCH 101, TLOSS 0.795, VLOSS 0.922, TOP1 78.19, TOP5 94.94, TIME 53.429\n",
      "EPOCH 102, TLOSS 0.782, VLOSS 0.918, TOP1 78.26, TOP5 94.85, TIME 53.164\n",
      "EPOCH 103, TLOSS 0.821, VLOSS 0.925, TOP1 78.23, TOP5 94.85, TIME 53.166\n",
      "EPOCH 104, TLOSS 0.799, VLOSS 0.915, TOP1 78.16, TOP5 94.87, TIME 53.254\n",
      "EPOCH 105, TLOSS 0.784, VLOSS 0.921, TOP1 78.25, TOP5 94.86, TIME 53.178\n",
      "EPOCH 106, TLOSS 0.802, VLOSS 0.918, TOP1 78.25, TOP5 94.86, TIME 53.323\n",
      "EPOCH 107, TLOSS 0.780, VLOSS 0.921, TOP1 78.12, TOP5 94.91, TIME 53.397\n",
      "EPOCH 108, TLOSS 0.782, VLOSS 0.918, TOP1 78.29, TOP5 94.84, TIME 53.439\n",
      "EPOCH 109, TLOSS 0.810, VLOSS 0.923, TOP1 78.16, TOP5 94.88, TIME 53.194\n",
      "EPOCH 110, TLOSS 0.795, VLOSS 0.924, TOP1 78.32, TOP5 94.83, TIME 53.395\n",
      "EPOCH 111, TLOSS 0.781, VLOSS 0.922, TOP1 78.02, TOP5 94.88, TIME 53.124\n",
      "EPOCH 112, TLOSS 0.802, VLOSS 0.917, TOP1 78.17, TOP5 95.00, TIME 53.328\n",
      "EPOCH 113, TLOSS 0.769, VLOSS 0.917, TOP1 78.18, TOP5 94.90, TIME 53.725\n",
      "EPOCH 114, TLOSS 0.797, VLOSS 0.918, TOP1 78.09, TOP5 94.92, TIME 53.514\n",
      "EPOCH 115, TLOSS 0.785, VLOSS 0.923, TOP1 78.43, TOP5 94.92, TIME 53.425\n",
      "EPOCH 116, TLOSS 0.775, VLOSS 0.918, TOP1 78.28, TOP5 94.90, TIME 53.410\n",
      "EPOCH 117, TLOSS 0.783, VLOSS 0.920, TOP1 78.16, TOP5 94.92, TIME 53.328\n",
      "EPOCH 118, TLOSS 0.804, VLOSS 0.920, TOP1 78.23, TOP5 94.95, TIME 53.475\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "EPOCH 119, TLOSS 0.792, VLOSS 0.918, TOP1 78.33, TOP5 94.91, TIME 53.391\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "EPOCH 120, TLOSS 0.797, VLOSS 0.925, TOP1 78.28, TOP5 94.87, TIME 53.459\n",
      "EPOCH 121, TLOSS 0.791, VLOSS 0.923, TOP1 78.16, TOP5 94.93, TIME 53.563\n",
      "EPOCH 122, TLOSS 0.774, VLOSS 0.920, TOP1 78.20, TOP5 94.89, TIME 53.713\n",
      "EPOCH 123, TLOSS 0.814, VLOSS 0.915, TOP1 78.37, TOP5 94.90, TIME 53.325\n",
      "EPOCH 124, TLOSS 0.787, VLOSS 0.916, TOP1 78.17, TOP5 94.95, TIME 53.564\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 125, TLOSS 0.793, VLOSS 0.918, TOP1 78.31, TOP5 94.94, TIME 53.524\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 126, TLOSS 0.785, VLOSS 0.921, TOP1 78.23, TOP5 94.91, TIME 53.645\n",
      "EPOCH 127, TLOSS 0.806, VLOSS 0.917, TOP1 78.22, TOP5 94.89, TIME 53.466\n",
      "EPOCH 128, TLOSS 0.771, VLOSS 0.913, TOP1 78.26, TOP5 94.88, TIME 53.594\n",
      "EPOCH 129, TLOSS 0.808, VLOSS 0.926, TOP1 78.15, TOP5 94.88, TIME 53.938\n",
      "EPOCH 130, TLOSS 0.769, VLOSS 0.911, TOP1 78.38, TOP5 94.98, TIME 53.854\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "EPOCH 131, TLOSS 0.782, VLOSS 0.912, TOP1 78.26, TOP5 94.86, TIME 54.048\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "EPOCH 132, TLOSS 0.770, VLOSS 0.921, TOP1 78.24, TOP5 94.85, TIME 54.108\n",
      "EPOCH 133, TLOSS 0.794, VLOSS 0.912, TOP1 78.13, TOP5 94.88, TIME 54.282\n",
      "EPOCH 134, TLOSS 0.790, VLOSS 0.920, TOP1 78.29, TOP5 94.84, TIME 54.046\n",
      "EPOCH 135, TLOSS 0.788, VLOSS 0.918, TOP1 78.18, TOP5 94.80, TIME 54.127\n",
      "EPOCH 136, TLOSS 0.788, VLOSS 0.915, TOP1 78.25, TOP5 94.95, TIME 53.990\n",
      "EPOCH 137, TLOSS 0.793, VLOSS 0.920, TOP1 78.24, TOP5 94.91, TIME 54.053\n",
      "EPOCH 138, TLOSS 0.787, VLOSS 0.916, TOP1 78.26, TOP5 94.88, TIME 54.324\n",
      "EPOCH 139, TLOSS 0.777, VLOSS 0.920, TOP1 78.19, TOP5 94.83, TIME 54.346\n",
      "EPOCH 140, TLOSS 0.768, VLOSS 0.916, TOP1 78.22, TOP5 94.93, TIME 54.126\n",
      "EPOCH 141, TLOSS 0.801, VLOSS 0.919, TOP1 78.33, TOP5 94.89, TIME 54.117\n",
      "EPOCH 142, TLOSS 0.774, VLOSS 0.912, TOP1 78.20, TOP5 94.94, TIME 54.085\n",
      "EPOCH 143, TLOSS 0.772, VLOSS 0.914, TOP1 78.29, TOP5 94.97, TIME 54.406\n",
      "EPOCH 144, TLOSS 0.782, VLOSS 0.915, TOP1 78.20, TOP5 94.86, TIME 54.486\n",
      "EPOCH 145, TLOSS 0.779, VLOSS 0.915, TOP1 78.28, TOP5 94.95, TIME 54.453\n",
      "EPOCH 146, TLOSS 0.787, VLOSS 0.919, TOP1 78.26, TOP5 95.00, TIME 54.548\n",
      "EPOCH 147, TLOSS 0.776, VLOSS 0.917, TOP1 78.13, TOP5 94.82, TIME 54.441\n",
      "EPOCH 148, TLOSS 0.797, VLOSS 0.917, TOP1 78.24, TOP5 94.90, TIME 54.578\n",
      "EPOCH 149, TLOSS 0.799, VLOSS 0.918, TOP1 78.33, TOP5 94.91, TIME 54.584\n",
      "EPOCH 150, TLOSS 0.773, VLOSS 0.912, TOP1 78.38, TOP5 94.87, TIME 54.535\n",
      "EPOCH 151, TLOSS 0.787, VLOSS 0.916, TOP1 78.25, TOP5 94.88, TIME 54.308\n",
      "EPOCH 152, TLOSS 0.774, VLOSS 0.916, TOP1 78.19, TOP5 94.89, TIME 54.316\n",
      "EPOCH 153, TLOSS 0.781, VLOSS 0.918, TOP1 78.25, TOP5 94.88, TIME 54.221\n",
      "EPOCH 154, TLOSS 0.794, VLOSS 0.914, TOP1 78.32, TOP5 94.93, TIME 54.163\n",
      "EPOCH 155, TLOSS 0.773, VLOSS 0.917, TOP1 78.35, TOP5 94.88, TIME 54.238\n",
      "EPOCH 156, TLOSS 0.785, VLOSS 0.919, TOP1 78.34, TOP5 94.95, TIME 54.198\n",
      "EPOCH 157, TLOSS 0.787, VLOSS 0.914, TOP1 78.10, TOP5 94.87, TIME 54.178\n",
      "EPOCH 158, TLOSS 0.770, VLOSS 0.909, TOP1 78.37, TOP5 94.98, TIME 54.059\n",
      "EPOCH 159, TLOSS 0.782, VLOSS 0.915, TOP1 78.25, TOP5 94.92, TIME 54.515\n",
      "EPOCH 160, TLOSS 0.799, VLOSS 0.920, TOP1 78.25, TOP5 94.89, TIME 54.622\n",
      "EPOCH 161, TLOSS 0.774, VLOSS 0.918, TOP1 78.20, TOP5 94.98, TIME 54.639\n",
      "EPOCH 162, TLOSS 0.788, VLOSS 0.919, TOP1 78.29, TOP5 94.83, TIME 54.793\n",
      "EPOCH 163, TLOSS 0.794, VLOSS 0.918, TOP1 78.30, TOP5 94.88, TIME 54.816\n",
      "EPOCH 164, TLOSS 0.789, VLOSS 0.924, TOP1 78.22, TOP5 94.94, TIME 54.803\n",
      "EPOCH 165, TLOSS 0.785, VLOSS 0.917, TOP1 78.30, TOP5 94.86, TIME 54.697\n",
      "EPOCH 166, TLOSS 0.780, VLOSS 0.917, TOP1 78.29, TOP5 94.90, TIME 54.629\n",
      "EPOCH 167, TLOSS 0.779, VLOSS 0.919, TOP1 78.26, TOP5 94.93, TIME 54.892\n",
      "EPOCH 168, TLOSS 0.764, VLOSS 0.921, TOP1 78.16, TOP5 94.89, TIME 54.988\n",
      "EPOCH 169, TLOSS 0.754, VLOSS 0.910, TOP1 78.17, TOP5 94.87, TIME 54.995\n",
      "EPOCH 170, TLOSS 0.781, VLOSS 0.916, TOP1 78.18, TOP5 94.89, TIME 54.916\n",
      "EPOCH 171, TLOSS 0.770, VLOSS 0.919, TOP1 78.26, TOP5 94.89, TIME 55.275\n",
      "EPOCH 172, TLOSS 0.775, VLOSS 0.920, TOP1 78.32, TOP5 94.99, TIME 54.749\n",
      "EPOCH 173, TLOSS 0.777, VLOSS 0.920, TOP1 78.25, TOP5 94.90, TIME 54.810\n",
      "EPOCH 174, TLOSS 0.787, VLOSS 0.919, TOP1 78.27, TOP5 94.86, TIME 54.481\n",
      "EPOCH 175, TLOSS 0.778, VLOSS 0.916, TOP1 78.33, TOP5 94.87, TIME 54.840\n",
      "EPOCH 176, TLOSS 0.767, VLOSS 0.918, TOP1 78.35, TOP5 94.98, TIME 54.662\n",
      "EPOCH 177, TLOSS 0.767, VLOSS 0.913, TOP1 78.19, TOP5 94.91, TIME 54.501\n",
      "EPOCH 178, TLOSS 0.793, VLOSS 0.918, TOP1 78.19, TOP5 94.86, TIME 54.378\n",
      "EPOCH 179, TLOSS 0.797, VLOSS 0.917, TOP1 78.30, TOP5 94.87, TIME 54.481\n",
      "EPOCH 180, TLOSS 0.791, VLOSS 0.918, TOP1 78.29, TOP5 94.88, TIME 54.410\n",
      "EPOCH 181, TLOSS 0.784, VLOSS 0.919, TOP1 78.28, TOP5 94.88, TIME 54.638\n",
      "EPOCH 182, TLOSS 0.764, VLOSS 0.914, TOP1 78.23, TOP5 94.84, TIME 54.559\n",
      "EPOCH 183, TLOSS 0.792, VLOSS 0.918, TOP1 78.16, TOP5 94.85, TIME 54.680\n",
      "EPOCH 184, TLOSS 0.778, VLOSS 0.915, TOP1 78.23, TOP5 94.85, TIME 54.352\n",
      "EPOCH 185, TLOSS 0.808, VLOSS 0.921, TOP1 78.19, TOP5 94.86, TIME 54.389\n",
      "EPOCH 186, TLOSS 0.791, VLOSS 0.921, TOP1 78.18, TOP5 94.93, TIME 54.637\n",
      "EPOCH 187, TLOSS 0.780, VLOSS 0.917, TOP1 78.22, TOP5 94.83, TIME 54.192\n",
      "EPOCH 188, TLOSS 0.782, VLOSS 0.915, TOP1 78.09, TOP5 94.85, TIME 54.403\n",
      "EPOCH 189, TLOSS 0.780, VLOSS 0.918, TOP1 78.22, TOP5 94.94, TIME 54.270\n",
      "EPOCH 190, TLOSS 0.794, VLOSS 0.914, TOP1 78.21, TOP5 94.98, TIME 54.372\n",
      "EPOCH 191, TLOSS 0.774, VLOSS 0.914, TOP1 78.16, TOP5 94.89, TIME 54.385\n",
      "EPOCH 192, TLOSS 0.788, VLOSS 0.919, TOP1 78.35, TOP5 94.90, TIME 54.543\n",
      "EPOCH 193, TLOSS 0.791, VLOSS 0.920, TOP1 78.30, TOP5 94.94, TIME 54.649\n",
      "EPOCH 194, TLOSS 0.800, VLOSS 0.917, TOP1 78.22, TOP5 94.87, TIME 55.028\n",
      "EPOCH 195, TLOSS 0.794, VLOSS 0.915, TOP1 78.20, TOP5 94.88, TIME 54.723\n",
      "EPOCH 196, TLOSS 0.775, VLOSS 0.916, TOP1 78.14, TOP5 94.95, TIME 54.945\n",
      "EPOCH 197, TLOSS 0.798, VLOSS 0.917, TOP1 78.18, TOP5 94.80, TIME 55.297\n",
      "EPOCH 198, TLOSS 0.786, VLOSS 0.917, TOP1 78.22, TOP5 94.93, TIME 55.154\n",
      "EPOCH 199, TLOSS 0.779, VLOSS 0.921, TOP1 78.17, TOP5 94.91, TIME 55.216\n",
      "EPOCH 200, TLOSS 0.778, VLOSS 0.915, TOP1 78.20, TOP5 94.93, TIME 55.309\n",
      "EPOCH 201, TLOSS 0.792, VLOSS 0.922, TOP1 78.24, TOP5 94.87, TIME 55.187\n",
      "EPOCH 202, TLOSS 0.779, VLOSS 0.917, TOP1 78.23, TOP5 94.84, TIME 54.964\n",
      "EPOCH 203, TLOSS 0.786, VLOSS 0.915, TOP1 78.15, TOP5 94.82, TIME 55.110\n",
      "EPOCH 204, TLOSS 0.786, VLOSS 0.920, TOP1 78.26, TOP5 94.92, TIME 55.280\n",
      "EPOCH 205, TLOSS 0.773, VLOSS 0.917, TOP1 78.35, TOP5 94.89, TIME 55.505\n",
      "EPOCH 206, TLOSS 0.772, VLOSS 0.917, TOP1 78.22, TOP5 94.85, TIME 55.005\n",
      "EPOCH 207, TLOSS 0.793, VLOSS 0.921, TOP1 78.24, TOP5 94.90, TIME 55.199\n",
      "EPOCH 208, TLOSS 0.772, VLOSS 0.918, TOP1 78.33, TOP5 94.94, TIME 55.721\n",
      "EPOCH 209, TLOSS 0.789, VLOSS 0.921, TOP1 78.13, TOP5 94.88, TIME 55.281\n",
      "EPOCH 210, TLOSS 0.792, VLOSS 0.920, TOP1 78.33, TOP5 94.96, TIME 55.193\n",
      "EPOCH 211, TLOSS 0.791, VLOSS 0.924, TOP1 78.15, TOP5 94.85, TIME 55.173\n",
      "EPOCH 212, TLOSS 0.780, VLOSS 0.919, TOP1 78.27, TOP5 95.01, TIME 55.104\n",
      "EPOCH 213, TLOSS 0.802, VLOSS 0.923, TOP1 78.39, TOP5 94.88, TIME 55.282\n",
      "EPOCH 214, TLOSS 0.772, VLOSS 0.921, TOP1 78.24, TOP5 94.92, TIME 55.243\n",
      "EPOCH 215, TLOSS 0.788, VLOSS 0.924, TOP1 78.17, TOP5 94.82, TIME 55.835\n",
      "EPOCH 216, TLOSS 0.790, VLOSS 0.917, TOP1 78.20, TOP5 94.86, TIME 55.852\n",
      "EPOCH 217, TLOSS 0.795, VLOSS 0.918, TOP1 78.27, TOP5 94.80, TIME 55.734\n",
      "EPOCH 218, TLOSS 0.783, VLOSS 0.924, TOP1 78.17, TOP5 94.91, TIME 56.133\n",
      "EPOCH 219, TLOSS 0.797, VLOSS 0.918, TOP1 78.25, TOP5 94.89, TIME 55.971\n",
      "EPOCH 220, TLOSS 0.786, VLOSS 0.919, TOP1 78.22, TOP5 94.80, TIME 56.026\n",
      "EPOCH 221, TLOSS 0.792, VLOSS 0.914, TOP1 78.27, TOP5 94.91, TIME 56.188\n",
      "EPOCH 222, TLOSS 0.767, VLOSS 0.912, TOP1 78.32, TOP5 94.86, TIME 55.849\n",
      "EPOCH 223, TLOSS 0.792, VLOSS 0.914, TOP1 78.26, TOP5 94.88, TIME 56.283\n",
      "EPOCH 224, TLOSS 0.773, VLOSS 0.914, TOP1 78.22, TOP5 94.89, TIME 56.489\n",
      "EPOCH 225, TLOSS 0.785, VLOSS 0.914, TOP1 78.31, TOP5 94.92, TIME 56.243\n",
      "EPOCH 226, TLOSS 0.781, VLOSS 0.917, TOP1 78.25, TOP5 94.88, TIME 56.472\n",
      "EPOCH 227, TLOSS 0.785, VLOSS 0.919, TOP1 78.29, TOP5 94.80, TIME 56.529\n",
      "EPOCH 228, TLOSS 0.777, VLOSS 0.922, TOP1 78.23, TOP5 94.92, TIME 56.273\n",
      "EPOCH 229, TLOSS 0.769, VLOSS 0.915, TOP1 78.10, TOP5 94.87, TIME 56.084\n",
      "EPOCH 230, TLOSS 0.786, VLOSS 0.918, TOP1 78.15, TOP5 94.90, TIME 56.289\n",
      "EPOCH 231, TLOSS 0.798, VLOSS 0.915, TOP1 78.21, TOP5 94.88, TIME 56.579\n",
      "EPOCH 232, TLOSS 0.810, VLOSS 0.922, TOP1 78.28, TOP5 94.90, TIME 56.839\n",
      "EPOCH 233, TLOSS 0.810, VLOSS 0.920, TOP1 78.23, TOP5 94.96, TIME 56.739\n",
      "EPOCH 234, TLOSS 0.788, VLOSS 0.919, TOP1 78.27, TOP5 94.92, TIME 56.821\n",
      "EPOCH 235, TLOSS 0.801, VLOSS 0.920, TOP1 78.26, TOP5 94.94, TIME 57.032\n",
      "EPOCH 236, TLOSS 0.808, VLOSS 0.917, TOP1 78.22, TOP5 94.85, TIME 56.968\n",
      "EPOCH 237, TLOSS 0.780, VLOSS 0.921, TOP1 78.14, TOP5 94.90, TIME 56.949\n",
      "EPOCH 238, TLOSS 0.789, VLOSS 0.923, TOP1 78.08, TOP5 94.86, TIME 56.742\n",
      "EPOCH 239, TLOSS 0.788, VLOSS 0.919, TOP1 78.37, TOP5 94.98, TIME 56.887\n",
      "EPOCH 240, TLOSS 0.777, VLOSS 0.918, TOP1 78.15, TOP5 94.94, TIME 57.040\n",
      "EPOCH 241, TLOSS 0.804, VLOSS 0.920, TOP1 78.24, TOP5 94.93, TIME 57.322\n",
      "EPOCH 242, TLOSS 0.774, VLOSS 0.915, TOP1 78.19, TOP5 94.96, TIME 57.284\n",
      "EPOCH 243, TLOSS 0.780, VLOSS 0.915, TOP1 78.09, TOP5 94.91, TIME 57.280\n",
      "EPOCH 244, TLOSS 0.768, VLOSS 0.920, TOP1 78.33, TOP5 94.95, TIME 57.236\n",
      "EPOCH 245, TLOSS 0.802, VLOSS 0.916, TOP1 78.19, TOP5 94.87, TIME 57.005\n",
      "EPOCH 246, TLOSS 0.781, VLOSS 0.919, TOP1 78.25, TOP5 94.94, TIME 56.952\n",
      "EPOCH 247, TLOSS 0.789, VLOSS 0.919, TOP1 78.17, TOP5 94.94, TIME 56.899\n",
      "EPOCH 248, TLOSS 0.779, VLOSS 0.917, TOP1 78.14, TOP5 94.92, TIME 56.732\n",
      "EPOCH 249, TLOSS 0.787, VLOSS 0.917, TOP1 78.22, TOP5 95.04, TIME 56.962\n"
     ]
    }
   ],
   "source": [
    "# DDP training routine inputs\n",
    "model_type = 'beta3'\n",
    "world_size = 6\n",
    "time_budget_mins = np.inf # minutes per trial\n",
    "nepochs = 250\n",
    "batch_size = 100\n",
    "accumulate = 1\n",
    "evaluate = True\n",
    "saving = 'final'\n",
    "\n",
    "# Import or define model parameters\n",
    "with open('nas_results/beta3/R224/trial_results/best_parameters.pkl', 'rb') as handle:\n",
    "    model_parameters = pickle.load(handle)\n",
    "model_parameters[\"R\"] = R\n",
    "model_parameters[\"output_size\"] = 100\n",
    "\n",
    "%run -i bte_ddp.py\n",
    "\n",
    "with open(f'experiment_results/{model_type}_result.pkl', 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "    \n",
    "# Unpack results object\n",
    "avg_epoch_train_time = [results[e][\"time\"] for e in results]\n",
    "train_loss_epoch = np.array([results[e][\"tloss\"] for e in results])\n",
    "valid_loss_epoch = np.array([results[e][\"vloss\"] for e in results])\n",
    "valid_top1accu_epoch = np.array([results[e][\"top1\"] for e in results])\n",
    "valid_top5accu_epoch = np.array([results[e][\"top5\"] for e in results])\n",
    "\n",
    "target = valid_top1accu_epoch.max()\n",
    "\n",
    "# Save trial result measures to disk for inspection later.\n",
    "payload = {\n",
    "    'R': R,\n",
    "    'time_budget_mins': time_budget_mins,\n",
    "    'params': model_parameters,\n",
    "    'tloss':train_loss_epoch,\n",
    "    'vloss':valid_loss_epoch,\n",
    "    'top1': valid_top1accu_epoch,\n",
    "    'top5': valid_top5accu_epoch,\n",
    "    'time': avg_epoch_train_time\n",
    "}\n",
    "\n",
    "with open(f'experiment_results/BetaNet3_384_results.pkl', 'wb') as handle:\n",
    "    pickle.dump(payload, handle)\n",
    "\n",
    "# Need to delete this because otherwise failed training is skipped and the file from last run is picked up instead.\n",
    "os.remove(f'experiment_results/{model_type}_result.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d7f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
